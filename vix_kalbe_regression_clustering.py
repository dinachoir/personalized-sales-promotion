# -*- coding: utf-8 -*-
"""VIX_kalbe_time-series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xm5DSBUYsESm4xydRCOs1UWHNTiTPTuX

# Regression

## Objectives :
Memprediksi total quantity harian dari product yang terjual menggunakan model machine learning ARIMA.

## Import Library
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from datetime import datetime
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from numpy import log
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from sklearn.metrics import mean_squared_error, mean_absolute_error
from statsmodels.tsa.arima_model import ARIMA
import statsmodels.api as sm

"""## Import Data"""

df_customer = pd.read_csv('/content/Case Study - Customer.csv', sep=';')
df_product = pd.read_csv('/content/Case Study - Product.csv', sep=';')
df_store = pd.read_csv('/content/Case Study - Store.csv', sep=';')
df_transaction = pd.read_csv('/content/Case Study - Transaction.csv', sep=';')
df_store.head()

"""## Data Preprocessing"""

df_customer.info()

df_product.info()

df_store.info()

df_transaction.info()

df_transaction['Date'] = pd.to_datetime(df_transaction['Date'], format="%d/%m/%Y")

df_transaction['CustomerID'] = df_transaction['CustomerID'].astype(str)
df_transaction['StoreID'] = df_transaction['StoreID'].astype(str)
df_store['StoreID'] = df_store['StoreID'].astype(str)
df_customer['CustomerID'] = df_customer['CustomerID'].astype(str)

df_merged = pd.merge(df_transaction, df_customer, on=['CustomerID'])
df_merged = pd.merge(df_merged, df_product.drop(columns=['Price']), on=['ProductID'])
df_merged = pd.merge(df_merged, df_store, on=['StoreID'])

df_merged.head()

df_merged.info()

df_regresi = df_merged.groupby(['Date']).agg({'Qty' : 'sum'}).reset_index()

df_regresi.head()

decomposed = seasonal_decompose(df_regresi.set_index(['Date']))

plt.figure(figsize=(10,8))

plt.subplot(311)
decomposed.trend.plot(ax=plt.gca())
plt.title('Trend')
plt.subplot(312)
decomposed.seasonal.plot(ax=plt.gca())
plt.title('Seasonality')
plt.subplot(313)
decomposed.resid.plot(ax=plt.gca())
plt.title('Residuals')

plt.tight_layout()

cut_off = round(df_regresi.shape[0] * 0.9)
df_train = df_regresi[:cut_off]
df_test = df_regresi[cut_off:].reset_index(drop=True)
df_train.shape, df_test.shape

plt.figure(figsize=(12,3))
sns.lineplot(data=df_train, x=df_train['Date'], y=df_train['Qty'])
sns.lineplot(data=df_test, x=df_test['Date'], y=df_test['Qty'])

"""## Modeling"""

res = adfuller(df_regresi['Qty'].dropna())
print('Augmented Dickey-Fuller Statistic: %f' % res[0])
print('p-value: %f' % res[1])

"""The ADF test's p-value is less than the significance level (0.05), then we will reject the null hypothesis and infer that the time series is definitely stationary."""

plot_acf(df_regresi['Qty'])
plt.show()
plot_pacf(df_regresi['Qty'])
plt.show()

def rmse(y_actual, y_pred):
  print(f'RMSE value {mean_squared_error(y_actual, y_pred)**0.5}')

def eval(y_actual, y_pred):
  rmse(y_actual, y_pred)
  print(f'MAE value {mean_absolute_error(y_actual, y_pred)}')

df_train = df_train.set_index('Date')
df_test = df_test.set_index('Date')

y = df_train['Qty']

ARIMA_model = sm.tsa.arima.ARIMA(y, order=(37,1,0))
ARIMA_model = ARIMA_model.fit()

y_pred = ARIMA_model.get_forecast(len(df_test))

y_pred_df = y_pred.conf_int()
y_pred_df['predictions'] = ARIMA_model.predict(start = y_pred_df.index[0], end = y_pred_df.index[-1])
y_pred_df.index = df_test.index
y_pred_out = y_pred_df['predictions']
eval(df_test['Qty'], y_pred_out)

plt.figure(figsize=(18,5))
plt.plot(df_train['Qty'])
plt.plot(df_test['Qty'])
plt.plot(y_pred_out, color='green', label='ARIMA Predictions')
plt.legend()

ARIMA_model.plot_diagnostics(figsize=(12,8))
plt.show()

"""# Clustering

## Data Preprocessing
"""

df_cluster = df_merged.groupby(['CustomerID']).agg({
    'TransactionID' : 'count',
    'Qty' : 'sum',
    'TotalAmount' : 'sum',
    'Age' : 'mean',
    'Gender' : 'mean',
    'Income' : 'mean'
    }).reset_index()

df_cluster.head()

df_cluster.drop(['CustomerID'], axis=1, inplace=True)

plt.figure(figsize=(4,3))
sns.heatmap(df_cluster.corr(), annot=True, fmt='.2f', linewidth=1)
plt.tight_layout()
plt.show()

df_cluster['F'] = df_cluster['TransactionID']
df_cluster['M'] = df_cluster['TotalAmount']
df_cluster['Q'] = df_cluster['Qty']

cols = ['F', 'M', 'Q']
plt.figure(figsize=(8, 3))
for i in range(len(cols)):
    plt.subplot(1, 3, i+1)
    sns.boxplot(y=df_cluster[cols[i]])
    plt.tight_layout()

df_scaled = df_cluster.filter(['F', 'M', 'Q'])

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df_scaled = scaler.fit_transform(df_scaled)
df_scaled = pd.DataFrame(df_scaled, columns=['F', 'M', 'Q'])
df_scaled.head()

sns.kdeplot(df_scaled['F'])
sns.kdeplot(df_scaled['M'])
sns.kdeplot(df_scaled['Q'])
plt.tight_layout()

df_scaled.describe().T

"""## Modeling"""

pd.set_option('display.max_columns', None)
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

inertia = []

for i in range(2, 11):
    kmeans = KMeans(n_clusters=i)#, random_state=0)
    kmeans.fit(df_scaled.values)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(7,4))
plt.title('Inertia Evaluation Score', fontsize=12)
sns.lineplot(x=range(2, 11), y=inertia, linewidth = 2)
sns.scatterplot(x=range(2, 11), y=inertia, s=100, linestyle='--')

(pd.Series(inertia) - pd.Series(inertia).shift(-1)) / pd.Series(inertia) * 100

kmeans = KMeans(n_clusters=3, random_state=42).fit(df_scaled)
df_scaled['cluster'] = kmeans.labels_

df_clustering = df_cluster.filter(['TransactionID','TotalAmount','Qty'])
df_clustering.columns = ['F', 'M', 'Q']
df_clustering['cluster'] = kmeans.labels_

pca = PCA(n_components=2)

pca.fit(df_scaled)
pcs = pca.transform(df_scaled)

df_pca = pd.DataFrame(data = pcs, columns = ['PC 1', 'PC 2'])
df_pca['cluster'] = df_scaled['cluster']
df_pca.sample(10)

"""## Visualization"""

fig, ax = plt.subplots(figsize=(10,8))
plt.title("2-D Visualization of Customer Clusters\nWih PCA", fontsize=15, weight='bold')
sns.scatterplot(y="PC 1",
                x="PC 2",
                hue="cluster",
                edgecolor='black',
                data=df_pca,
                palette=['blue','orange','green'],
                s=160,
                ax=ax)

fig = plt.figure(figsize=(15,10))
ax = fig.add_subplot(111, projection='3d')
plt.title("3-D Visualization of Customer Clusters", fontsize=15, weight='bold')
ax.scatter(df_clustering['Q'][df_clustering.cluster == 2], df_clustering['F'][df_clustering.cluster == 2], df_clustering['M'][df_clustering.cluster == 2], c='green', s=100, edgecolor='black', label='High Value')
ax.scatter(df_clustering['Q'][df_clustering.cluster == 1], df_clustering['F'][df_clustering.cluster == 1], df_clustering['M'][df_clustering.cluster == 1], c='orange', s=100, edgecolor='black', label='Low Value')
ax.scatter(df_clustering['Q'][df_clustering.cluster == 0], df_clustering['F'][df_clustering.cluster == 0], df_clustering['M'][df_clustering.cluster == 0], c='blue', s=100, edgecolor='black', label='Middle Value')

plt.xlabel('Quantity')
plt.ylabel('Total Transaction')
ax.set_zlabel('Total Amount')
plt.legend(title='Cluster', loc='upper left')
plt.show()

"""## Interpretation"""

df_cluster['Cluster'] = kmeans.labels_

df_cluster.head(5)

for i in df_cluster.columns:
    plt.figure(figsize=(30,5))
    for j in range(3):
        plt.subplot(1,3,j+1)
        cluster = df_cluster[df_cluster['Cluster']==j]
        cluster[i].hist(bins=20)
        plt.title('{}  \nCluster  {}'.format(i,j))

    plt.show()

num_cols = ['TransactionID', 'Qty', 'TotalAmount']
#cat_cols = df.select_dtypes(include = ['object','category']).columns

display(df_cluster.groupby(['Cluster']).describe())

customer = df_cluster.groupby('Cluster')['Cluster'].count()/len(df_cluster)
customer = customer.to_frame().rename(columns={'Cluster':'%Customer'}).sort_values(by='%Customer',ascending=False).reset_index()

plt.figure(figsize=(7, 4))
ax = sns.barplot(x='Cluster', y='%Customer', data=customer, palette='gist_earth')
plt.title('The majority of customer is Middle Value\n followed by Low Value Customers', fontweight='bold')
plt.bar_label(ax.containers[0], ['46.3%', '28.6%', '24.8%'], label_type='center',
             color='white', fontweight='bold')
plt.xticks(np.arange(3), ['0\nMiddle value\nCustomer', '1\nLow value\nCustomer', '2\nHigh value\nCustomer'])
plt.yticks(None)
plt.xlabel(None)
ax.axes.yaxis.set_visible(False)
sns.despine(left=True)

plt.figure(figsize=(7, 4))
ax = sns.boxplot(y=df_cluster['TotalAmount'], x=df_cluster['Cluster'], palette='gist_earth')
plt.xticks(np.arange(3), ['0\nMiddle value\nCustomer', '1\nLow value\nCustomer', '2\nHigh value\nCustomer'])
plt.xlabel(None)
plt.ylabel('Total Amount')

plt.figure(figsize=(7, 4))
ax = sns.boxplot(y=df_cluster['TransactionID'], x=df_cluster['Cluster'], palette='gist_earth')
plt.xticks(np.arange(3), ['0\nMiddle value\nCustomer', '1\nLow value\nCustomer', '2\nHigh value\nCustomer'])
plt.xlabel(None)
plt.ylabel('Total Transaction')

plt.figure(figsize=(7, 4))
ax = sns.boxplot(y=df_cluster['Qty'], x=df_cluster['Cluster'], palette='gist_earth')
plt.xticks(np.arange(3), ['0\nMiddle value\nCustomer', '1\nLow value\nCustomer', '2\nHigh value\nCustomer'])
plt.xlabel(None)
plt.ylabel('Quantity')